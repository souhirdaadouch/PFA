# -*- coding: utf-8 -*-
"""BertPFA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xEQ-Vvp5w5Xu2c1z5KpGNx_nQFUy9xnX
"""

# Commented out IPython magic to ensure Python compatibility.
!pip install -q -U watermark
!pip install -qq transformers
!pip install PyDrive
# %reload_ext watermark
!pip3 install --upgrade tensorflow-gpu
# %watermark -v -p numpy,pandas,torch,transformers

# Commented out IPython magic to ensure Python compatibility.
import transformers

import tensorflow as tf

import torch

from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup

import torch
import json

import numpy as np

import pandas as pd

import seaborn as sns

from pylab import rcParams

import matplotlib.pyplot as plt

from matplotlib import rc

from sklearn.model_selection import train_test_split

from sklearn.metrics import confusion_matrix, classification_report

from collections import defaultdict

from textwrap import wrap

from torch import nn, optim

from torch.nn import functional as F


from torch.utils.data import Dataset, DataLoader

import os.path
 
import nltk

from nltk.corpus import stopwords

from nltk.tokenize import word_tokenize

from nltk.stem import PorterStemmer
import re

from os import path

# %matplotlib inline

# %config InlineBackend.figure_format='retina'

sns.set(style='whitegrid', palette='muted', font_scale=1.2)

HAPPY_COLORS_PALETTE = ["#01BEFE", "#FFDD00", "#FF7D00", "#FF006D", "#ADFF02", "#8F00FF"]

sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))

rcParams['figure.figsize'] = 12, 8

RANDOM_SEED = 42

np.random.seed(RANDOM_SEED)

torch.manual_seed(RANDOM_SEED)

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

nltk.download('punkt')
nltk.download('stopwords')

# test CUDA support for your Tensorflow installation, you can run the following command in the shell
tf.test.is_built_with_cuda()

#confirm that the GPU is available to Tensorflow
tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None)

#Extraction of the users captions and hashtags into columns of the dataframe

#Stemming:
def stem(df):
    stemmer= PorterStemmer()
    tokensList =[]
    captionList=[]
    caption=''
    for i in range(len(df)):
        caption=''
        tokensList=word_tokenize(df['captions'][i])
        #We wil ignore hashtags , so we save prev word , if it's a '#' we ignore the next word beacuase # is considered as a word
        # so we must ignore the word after '#'
        
        #this for loop will initilize prevWord
        for word in tokensList:
            prevWord=word
            break
            
        #here we will loop after words and ignore # , we will not execute stemmer on hashtags 
        #print(tokensList)
        for word in tokensList:
            if prevWord=='#':
                #print(word)
                caption +=' '+ word 
            else:
                if not(word.startswith('#')):
                    #print(stemmer.stem(word))
                    stemmer.stem(word)
                    caption +=' '+ stemmer.stem(word) 
            prevWord=word
        captionList.append(caption)
    for i in range(len(df['captions'])):
        df['captions'][i]=captionList[i]
    return df

#IMPORTANT ! CAPTION List contains captions without '#' with steemed word except hashtags

def remove_specials(df):
#Removing all the special characters :
    for i in range(len(df)):
        df['captions'][i]=re.sub('[^A-Za-z0-9]+', ' ', df['captions'][i])
       
    return df
#Removing the emojis:
def remove_emoji(string): 
    emoji_pattern = re.compile("["
                               u"\U0001F600-\U0001F64F"  # emoticons
                               u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                               u"\U0001F680-\U0001F6FF"  # transport & map symbols
                               u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                               u"\U00002500-\U00002BEF"  # chinese char
                               u"\U00002702-\U000027B0"
                               u"\U00002702-\U000027B0"
                               u"\U000024C2-\U0001F251"
                               u"\U0001f926-\U0001f937"
                               u"\U00010000-\U0010ffff"
                               u"\u2640-\u2642"
                               u"\u2600-\u2B55"
                               u"\u200d"
                               u"\u23cf"
                               u"\u23e9"
                               u"\u231a"
                               u"\ufe0f"  # dingbats
                               u"\u3030"
                               "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'',string)

#Removing all the stop words:   
def remove_stop_words(df):
    stop_words = set(stopwords.words('english'))
    for i in range(len(df)):
        tokens = word_tokenize(df['captions'][i])
        df['captions'][i] = [j for j in tokens if not j in stop_words]
    return df

def listToString(s):  
    
  #initialize an empty string 
    str1 = ""  
    # traverse in the string   
    for ele in s:  
        str1 = str1 + ' ' + ele   
    # return string   
    return str1  
def lowercase(df):
    df['captions']=df['captions'].str.lower()
    return df

#convert our JSON dataset to a pandas  dataframe
openfile=open('/content/sample_data/data_set.JSON')
jsondata=json.load(openfile)
df1=pd.DataFrame(jsondata)
df1= df1.transpose()
#droping the unuseful columns 
df1=df1.drop(['biography','full_name','business_category_name','profile_pic_url','followers','followees'],axis=1)
df1.head()

#creation of a list of lists
#liste[0]=[interest of post0,captions of post0]
liste=[]
for i in range(len(df1)):
        for j in range(len(df1['posts'][i])):
            liste1=[]
            post ='post'+str(j)
            if df1['posts'][i][post]['caption'] is not None:
               liste1.append(df1['interests'][i])
               liste1.append(df1['posts'][i][post]['caption'])
               liste.append(liste1)

#Creation of our final dataframe ( interests and the captions per post)
df = pd.DataFrame(liste, columns = ['interests', 'captions']) 
df.captions.values[0]

#Preporocessing of our final dataframe
#df.captions.values[0]
df=lowercase(df)
df=remove_specials(df)
for i in range(len(df)):
  df['captions'][i]=remove_emoji(df['captions'][i])   
df=remove_stop_words(df)
df.captions.values[0]

df.captions.values[774]

#Converting every captions content to a string
for i in range(len(df)):
  df['captions'][i]=listToString(df['captions'][i])

#Selecting a single  interest  for every post
for i in range(len(df)):
     liste=df.interests.values[i].split(',')
     df['interests'][i]=liste[0]
df.interests.values

#Creation of a new column 'category' that represents the id of every interest
possible_interests= df.interests.unique()
possible_interests
interests_dict={}
for index, filtred_interest in enumerate(possible_interests):
    interests_dict[filtred_interest]=index
df['category']=df.interests.replace(interests_dict)
df.head()

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',do_lower_case=True)

#total number of interests
len(possible_interests)

tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
token_lens = []

for txt in df.captions:
  txt=str(txt)
  if txt!=None:
    tokens = tokenizer.encode(txt, max_length=512)
    token_lens.append(len(tokens))

#encoding our labels (captions, category)
class Input_DataLoader(Dataset):

  def __init__(self, reviews, targets, tokenizer, max_len):
    self.reviews = reviews
    self.targets = targets
    self.tokenizer = tokenizer
    self.max_len = max_len
  
  def __len__(self):
    return len(self.reviews)
  
  def __getitem__(self, item):
    review = str(self.reviews[item])
    target = self.targets[item]

    encoding = self.tokenizer.encode_plus(
      review,
      add_special_tokens=True,
      max_length=self.max_len,
      return_token_type_ids=False,
      pad_to_max_length=True,
      return_attention_mask=True,
      return_tensors='pt',
    )

    return {
      'review_text': review,
      'input_ids': encoding['input_ids'].flatten(),
      'attention_mask': encoding['attention_mask'].flatten(),
      'targets': torch.tensor(target, dtype=torch.long)
    }

max_length=150

#Split our dataframe into a train, validation and a test dataframe
df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)
df_val, df_test = train_test_split(df_test, test_size=0.5, random_state=RANDOM_SEED)

def create_data_loader(df, tokenizer, max_len, batch_size):
  ds =Input_DataLoader(
    reviews=df.captions.to_numpy(),
    targets=df.category.to_numpy(),
    tokenizer=tokenizer,
    max_len=max_len
  )

  return DataLoader(
    ds,
    batch_size=batch_size,
    num_workers=4
  )

BATCH_SIZE =16

#creating  train, val, test dataloader
train_data_loader = create_data_loader(df_train, tokenizer, max_length, BATCH_SIZE)
val_data_loader = create_data_loader(df_val, tokenizer, max_length, BATCH_SIZE)
test_data_loader = create_data_loader(df_test, tokenizer, max_length, BATCH_SIZE)

data = next(iter(train_data_loader))
data.keys()

print(data['input_ids'].shape)
print(data['attention_mask'].shape)
print(data['targets'].shape)

bert_model = BertModel.from_pretrained('bert-base-cased')

class InterestClassifier(nn.Module):

  def __init__(self, n_classes):
    super(InterestClassifier, self).__init__()
    self.bert = BertModel.from_pretrained('bert-base-cased')
    self.drop = nn.Dropout(p=0.3)
    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)
  
  def forward(self, input_ids, attention_mask):
    _, pooled_output = self.bert(
      input_ids=input_ids,
      attention_mask=attention_mask
    )
    output = self.drop(pooled_output)
    return self.out(output)

#Creating our model
model = InterestClassifier(len(possible_interests))
model = model.to(device)

input_ids = data['input_ids'].to(device)
attention_mask = data['attention_mask'].to(device)

F.softmax(model(input_ids, attention_mask), dim=1)

EPOCHS = 8

optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
total_steps = len(train_data_loader) * EPOCHS

scheduler = get_linear_schedule_with_warmup(
  optimizer,
  num_warmup_steps=0,
  num_training_steps=total_steps
)

loss_fn = nn.CrossEntropyLoss().to(device)

def train_epoch(
  model, 
  data_loader, 
  loss_fn, 
  optimizer, 
  device, 
  scheduler, 
  n_examples
):
  model = model.train()

  losses = []
  correct_predictions = 0
  
  for d in data_loader:
    input_ids = d["input_ids"].to(device)
    attention_mask = d["attention_mask"].to(device)
    targets = d["targets"].to(device)

    outputs = model(
      input_ids=input_ids,
      attention_mask=attention_mask
    )

    _, preds = torch.max(outputs, dim=1)
    loss = loss_fn(outputs, targets)

    correct_predictions += torch.sum(preds == targets)
    losses.append(loss.item())

    loss.backward()
    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
    optimizer.step()
    scheduler.step()
    optimizer.zero_grad()

  return correct_predictions.double() / n_examples, np.mean(losses)

def eval_model(model, data_loader, loss_fn, device, n_examples):
  model = model.eval()

  losses = []
  correct_predictions = 0

  with torch.no_grad():
    for d in data_loader:
      input_ids = d["input_ids"].to(device)
      attention_mask = d["attention_mask"].to(device)
      targets = d["targets"].to(device)

      outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask
      )
      _, preds = torch.max(outputs, dim=1)

      loss = loss_fn(outputs, targets)

      correct_predictions += torch.sum(preds == targets)
      losses.append(loss.item())

  return correct_predictions.double() / n_examples, np.mean(losses)

#Training 
history = defaultdict(list)
best_accuracy = 0

for epoch in range(EPOCHS):

  print(f'Epoch {epoch + 1}/{EPOCHS}')
  print('-' * 10)

  train_acc, train_loss = train_epoch(
    model,
    train_data_loader,    
    loss_fn, 
    optimizer, 
    device, 
    scheduler, 
    len(df_train)
  )

  print(f'Train loss {train_loss} accuracy {train_acc}')

  val_acc, val_loss = eval_model(
    model,
    val_data_loader,
    loss_fn, 
    device, 
    len(df_val)
  )

  print(f'Val   loss {val_loss} accuracy {val_acc}')
  print()

  history['train_acc'].append(train_acc)
  history['train_loss'].append(train_loss)
  history['val_acc'].append(val_acc)
  history['val_loss'].append(val_loss)

#Training history 
plt.plot(history['train_acc'], label='train accuracy')

plt.title('Training history')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.ylim([0, 1]);

#Accuracy of our test model 
test_acc, _ = eval_model(
  model,
  test_data_loader,
  loss_fn,
  device,
  len(df_test)
)

test_acc.item()

def get_predictions(model, data_loader):
  model = model.eval()
  
  review_texts = []
  predictions = []
  prediction_probs = []
  real_values = []

  with torch.no_grad():
    for d in data_loader:

      texts = d["review_text"]
      input_ids = d["input_ids"].to(device)
      attention_mask = d["attention_mask"].to(device)
      targets = d["targets"].to(device)

      outputs = model(
        input_ids=input_ids,
        attention_mask=attention_mask
      )
      _, preds = torch.max(outputs, dim=1)

      probs = F.softmax(outputs, dim=1)

      review_texts.extend(texts)
      predictions.extend(preds)
      prediction_probs.extend(probs)
      real_values.extend(targets)

  predictions = torch.stack(predictions).cpu()
  prediction_probs = torch.stack(prediction_probs).cpu()
  real_values = torch.stack(real_values).cpu()
  return review_texts, predictions, prediction_probs, real_values

y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(
  model,
  test_data_loader
)

#Classification Report
print(classification_report(y_test, y_pred,target_names=possible_interests))

#Confusion Matrix
def show_confusion_matrix(confusion_matrix):
  hmap = sns.heatmap(confusion_matrix, annot=True, fmt="d", cmap="Blues")
  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')
  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')
  plt.ylabel('True Interest')
  plt.xlabel('Predicted Interests');

cm = confusion_matrix(y_test, y_pred)
df_cm = pd.DataFrame(cm, index=possible_interests, columns=possible_interests)
show_confusion_matrix(df_cm)

captions_text='I love reading books magazines novel I write many books '

encoded_captions = tokenizer.encode_plus(
  captions_text,
  max_length=max_length,
  add_special_tokens=True,
  return_token_type_ids=False,
  pad_to_max_length=True,
  return_attention_mask=True,
  return_tensors='pt',
)

#example of a prediction 
input_ids = encoded_captions['input_ids'].to(device)
attention_mask = encoded_captions['attention_mask'].to(device)

output = model(input_ids, attention_mask)
_, prediction = torch.max(output, dim=1)

print(f'Captions_text: {captions_text}')
print(f'Interest  : {possible_interests[prediction]}')